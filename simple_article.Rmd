---
title: "A Statistical Argument For Settling: a foray into extreme value theory"
author: "David Marx"
date: "October 10, 2015"
output: html_document
---

# High Standards

The other day I found a post in an online relationship forum in which the author had stumbled across a text conversation between his fiancee and one of her girlfriends he wasn't meant to see. In the conversation, she belittled his sexual performance and talked up her ex. The author of the post was reasonably distressed, but more than feeling disrespected, he insisted that it was extremely important to him to know that he was **the best lover she'd ever had**. 

Now personally, I don't think that's a fair standard, and I started thinking about how I could construct a statistical argument to demonstrate this. I respect that there is definitely a huge psychological component here, but for the purposes of the thought experiment: let's focus on the math.

# Formalizing the question

Someone has had $n$ sexual partners. Assume it is possible to attribute some kind of score to each partners' sexual ability, such that we can rank partners relative to this rating and clearly identify the best sexual partner thus far observed. If things don't work out, how many more people would you need to sleep with before the previous "high score" is met or surpassed?

Let's generalize the problem by restating it mathematically. 

1. Let $X \sim f(x;\theta)$ with CDF $F(x;\theta)$
2. Consider an i.i.d. sample of size $n$ drawn from $X$: $\{X_1, X_2, \ldots X_n \}$
3. Let $X_{(n)}$ denote the max observation in the above sample, i.e. $\mathop{\max} (X_1, X_2, \ldots X_n)$

We are interested in finding the expected wait time until we observe a value greater than the previously observed max, *given the number of samples we've already observed*. Concretely:

Find the quantity $E[j|n]$ such that $p(j|n) = p(X_{n+j} \ge X_{(n)} )$

# The distribution of $X_{(n)}$

The max value of a sample is less than or equal to all observations. Therefore, we can take product of the CDF of the generating distribution to get the CDF for the distribution of X_{(n)}.
Following the notation defined earlier: 

$$
\begin{align}
  P(X_{(n)} \le x) &= P(X_1 \le x, X_2 \le x, \ldots X_n \le x) \\
                   &= \prod_{i=1}^n P(X_i \le x) \\
                   &= \prod_{i=1}^n F(x) \\
                   &= F(x)^n
\end{align}
$$ 

The PDF for $X_{(n)}$ is therefore:

$$
\begin{align}
P(x = X_{(n)}) &= \frac{d}{dx} P(X_{(n)} \le x)  \\
               &= \frac{d}{dx} F(x)^n \\ 
               &= n F(x)^{n-1} f(x) 
\end{align}
$$

As the size of our sample ($n$) grows, the value taken by $X_{(n)}$ is expected to grow as well. Let's visualize this behavior by plotting the PDF for several values of $n$.

```{r, echo=FALSE}
fn = function(x,n) n * pnorm(x)^(n-1) * dnorm(x)

xv = seq(0,10,.001)
plot(0, 0, 
     xlim=c(0,5), 
     ylim=c(0, max(fn(xv,1e5))), 
     type='n', 
     main="Distribution of max(X) \nfor increasing values of n",
     xlab='x',
     ylab='P(max(X)=x|n)'
     )
dummy = sapply(1:5, function(x) lines(xv, fn(xv,10^x), col=x))
legend('topleft',paste0('n=1e',1:5), lty=1, col=1:5)
```

Observe that $X_{(n)}$ grows very slowly as we increase $n$: each new value of $n$ in the above plot is a full order of magnitude larger than the previous series.

Because $X_{(n)}$ is itself a random variable, it will simplify our problem a bit if we proceed by assuming we have observed the a value of $X_{(n)}|n$, i.e. considering fixed values for the max instead of the distribution of the max given the size of the sample. We'll come back to this problem formulation later, but let's work up to that by considering this simplified version of the problem first: we are interested in finding the exected wait time until we observe a value greater than the previously observed max value, given a concrete score associated with each observation.

The new problem formulation is: find the quantity $E[k|m]$ such that $p(k|m) = p(X_{n+k} \ge X_{(n)}| m=X_{(n)} , X_{(i)} \le X_{(n)} \forall (i\in {0:k-1}))$

# The Geometric Distribution

If we think of the event of finding a new max value as a "success", then we're trying to model the number of observations, $j$, until the first success after a series of failures. In other words, we need the geometric distribution, which has pdf:

$$dgeom(j,p) = p(1-p)^{j-1}$$

where $p$ is the probability of success. This PDF should make a lot of sense intuitively. If $p$ is the success probability, then $1-p$ is the failure probability. We want to know the probability that after $j$ observations, we've had $j-1$ failures and one success. Assuming successes and failures are independent events, we can calculate the joint probability by taking the products of the probabilities of each indpendent event, giving us the geometric PDF described above. 

Since we defined a "success" as achieving a score greater than or equal to the previously observed max value, $m$, we can extract this probability from the right tail of the PDF: the survival function $R(x)$ i.e. the complement of the CDF: $R(m)=1-F(m)$

```{r plot_norm_pdf_tail, echo=FALSE}
xv = seq(-4, 4, .01)
yv = dnorm(xv)
m = 1.5
i=which.max(xv==m)
j=length(xv)
yshift=.1*max(yv)
plot(xv, yv, type='l', xlab="", ylab="", axes=0, ylim=c(-yshift, max(yv)), main="Right tail of PDF = P(x>m)")
polygon(x=c(xv[i], xv[i:j], xv[j]), y= c(0, yv[i:j], 0), col="gray")
#abline(v=m, lty=2)
lines(c(m,m), c(-yshift*.5,max(yv)), lty=2)
#text(m+.5, dnorm(m), 'm')
#text(m, -yshift, 'm')
text(m, -yshift, expression(X[(n)]))
```

```{r}
Fn = function(x, n) pnorm(x)^n
R = function(x,n) 1-Fn(x,n)

xv = seq(-10,10,.001)
plot(0, 0, 
     xlim=c(0,5), 
     ylim=c(0, max(R(xv,1e5))), 
     type='n', 
     main="Survival Function for max(X)\nfor increasing values of n",
     xlab='x',
     ylab='R(x) = 1-F(x;n)'
     )
dummy = sapply(1:5, function(x) lines(xv, R(xv,10^x), col=x))
legend('bottomleft',paste0('n=1e',1:5), lty=1, col=1:5)
```


# Deriving the expectation of the geometric distribution 

The expectation of the geometric distribution is $\frac{1}{p}$. You can look this up on wikipedia or any math/stats textbook, but just for fun I'm going to derive this expectation here. You can just skip the rest of this section if you're not interested in this derivation.

Let $P(i) = p(1-p)^{i-1}$. 

Let $q=1-p$ such that $P(i) = (1-q)q^{i-1}$

$$
\begin{align}
E[j] &= \sum_{i=1}^{\infty} i (1-q) q^{i-1} \\
     &= \sum_{i=1}^{\infty} i (q^{i-1} - q^i) \\
     &= \sum_{i=1}^{\infty} i q^{i-1} - \sum_{i=1}^{\infty} iq^i \\
     &= \frac{1}{q}\sum_{i=1}^{\infty} i q^i - \sum_{i=1}^{\infty} iq^i \\
     &= \left(\frac{1}{q} - 1\right) \sum_{i=1}^{\infty} iq^i \\
     &= \left(\frac{1}{q} - 1\right) \left( \frac{q}{(1-q)^2} \right) \\
     &= \frac{1}{(1-q)^2} - \frac{q}{(1-q)^2} \\
     &= \frac{1-q}{(1-q)^2} \\
     &= \frac{1}{1-q} \\
     &= \frac{1}{p}
\end{align}
$$


See? That wasn't so bad. Like I said, $E[j]=\frac{1}{p}$.

Back to the problem.

# Expected time to next max value

By transforming the problem representation into a geometric random variable, we can infer that $p(i|m)=\frac{1}{1-F(m)}$. This should actually make a lot of sense intuitively. Recall the frequentist interpretation of probability:

$$P(x) \approx \frac{count(\text{# of events})}{count(\text{# of trials})}$$

Let's imagine that for some collection of $n$ samples with $X_{(n)}=m$, $R_{X_{(n)}}(m)=\frac{1}{t}$. The frequentist interpretation of probability essentially asserts that our observed max $m$ is a $1$ in $t$ observation, and so it'll probably take another $t$ observations before we see another observation at least as large. 

We've come a long way, but we still have the problem that we don't actually know $m$, and if we did we don't really have access to the CDF. We could construct some sort of measure and try to fit a CDF experimentally, but we can get pretty far analytically.

The missing piece to our solution is an estimator for $m$ given the number of samples we've observed. But, if you think about it, we don't really even need this: what we *really* need, is an estimator for $p$. For this, we can just fall back on the frequentist interpretation of probability. Empirically, the max observation in a sample of $n$ observations is a $1$ in $n$ observation, so our empirical estimate of seeing a value at least as large (its CDF complement) is therefore just $\frac{1}{n}$. 

This leaves us with the very simple heuristic that $E[j|n] = n$, assuming that $n$ is a reasonable approximation for $F(m)$.

Which, as we'll see in a moment: it's not (for large values of $n$).


## Putting it all together

From earlier, we have that the max of an i.i.d. sample of size $n$ with $CDF=F(x)$ is $F(x)^n$. We jumped through a lot of hoops to calculate $E[F(x)^n]$, but it's pretty easy to numerically estimate the *median* of this statistic, or any quantile.

$$X_{(n)} \sim \frac{d}{dx} F_X(x)^n \text{ s.t. } F_{X_{(n)}} = F_X(x)^n$$

$$E[X_{(n)}] \approx F^{-1}_{X_{(n)}}(.5)$$


```{r analytic_solution1}
xv = seq(-4,4, length.out=1e5)

N=1:20
q=c(.025, .5, .975)
test = sapply(N, function(n) pnorm(xv)^n)
CI_true = apply(test, 2, function(x) sapply(q, function(qi) xv[which.min(qi>x)]))
plot(0, 0, type='n', xlim=c(min(N), max(N)), ylim=c(min(CI_true), ylim=max(CI_true)))
lines(N, CI_true[1,], col='red', lty=2)
lines(N, CI_true[2,])
lines(N, CI_true[3,], col='red', lty=2)
```

If we plug the result back into the survival function, we get the probability of observing a quantity at least this large given the generating distribution. If we plug the resulting probability into the expected value of the geometric distribution, $\frac{1}{p}$, we get the expected time until we see an observation of at least this magnitude. 

$$p_n =F_X \left( E[X_{(n)}] \right) =  F_X \left( F^{-1}_{X_{(n)}}(.5) \right)$$

$$T_n=\frac{1}{1-p_n}=\frac{1}{1 - F_X \left( F^{-1}_{X_{(n)}}(.5) \right)}$$

This statistic $T_n$ gives us an approximation to the expected number of observations until we see a new max value, given the number of observations we've already observed.


```{r analytic_solution2}
CI2 = 1/(1-pnorm(CI_true))
plot(0, 0, type='n', 
     xlim=c(min(N), max(N)), 
     ylim=c( min(CI2), max(CI2) )
     )

lines(N, CI2[1,], col='red', lty=2)
lines(N, CI2[2,])
lines(N, CI2[3,], col='red', lty=2)
```



## Monte carlo simulation of mean

Let's run some simulations to test our math. To do this, we'll draw a new sample of size $n$ each iteration of the simulation and evaluate the max value for that sample. This will give us a distribution for the max at any given value of $n$ we're interested in.

```{r def_max_given_n}
max_sim_given_n = function(n, reps, fx=rnorm, values=FALSE, mu=TRUE, sigma=TRUE){
  replicate(reps, max(fx(n)))
}
```

Now let's run simulations over a large range of $n$ values and visualize the simulated expectation and 95% CI. I'm going to use the median instead of the mean as my simulated expectation to simplify my code a bit. It shouldn't really make much of a difference, but it's worth mentioning since formally monte carlo simulation generally uses the mean.

```{r sim_plot_max_given_n}
#totx=2e3, reps=2e2 -> 85.63 seconds
#totx=1e3, reps=2e2 -> 15.29 seconds
#totx=1e3, resp=1e2 ->  7.8 seconds
#totx=1e2, reps=1e2 ->  0.14 seconds
totx = 1e3 ## 11s ##2e3 
xv_ci = 1:totx
niters=2e2
system.time(CI <- sapply(xv_ci, function(x) {
  X = max_sim_given_n(x, niters)
  c(quantile(X, c(.025, .5, .975)), mean(X))
}))
plot(0,0, type='n', xlab='log(n)', ylab='E[X_(n)]', xlim=c(0,max(log(xv_ci))), ylim=c(0,max(CI)))
lines(log(xv_ci), CI[2,])
lines(log(xv_ci), CI[1,], col='red', lty=2)
lines(log(xv_ci), CI[3,], col='red', lty=2)
lines(log(xv_ci), CI[4,], col='blue', lty=2)
```

Let's plot the analytic solution and the simulation results together to see how they line up.

```{r}

plot(0, 0, type='n', 
     xlim=c(min(N), max(N)), 
     ylim=c( min(CI_true), max(CI_true) )
     )

lines(N, CI_true[1,], col='red', lty=2)
lines(N, CI_true[2,])
lines(N, CI_true[3,], col='red', lty=2)

lines(N, CI[2,N], col='blue', lty=2)
lines(N, CI[1,N], col='blue', lty=2)
lines(N, CI[3,N], col='blue', lty=2)
lines(N, CI[4,N], col='blue')
```

The simulation appears to corroborate our math. Also, we've qualitatively confirmed that this distribution is not skewed. This appeared to be the case from our earlier plots, but here we can see empirically that the median is approximately equal to the mean.

## Simulating the distribution of wait times

There is a famous result in probability called *Jensen's inequality*, which states that:

$$g(E[X]) \le E[g(X)]$$

Above, we found $E[j|X_{(n)}]$ such that $j$ is the wait time to the next max value (to exceed the max value already observed) for the given sample of size $n$: $X_{(n)}$. Furthermore, we demonstrated that we can use the transformation $g(x) = \frac{1}{1-pnorm(x)}$ to find the wait time to the next max value given some previously observed max value.

We can estimate this waiting time by plugging the analytic solution found above into the $g(x)$ transformation:


```{r}
xv=1:20
yv = 1/(1-pnorm(CI_true))
plot(0, 0, type='n', 
     xlim=c(min(xv), max(xv)), 
     ylim=c(0, max(yv) )
)
lines(N, yv[1,], col='red')
lines(N, yv[2,])
lines(N, yv[3,], col='red')
```

But this is really $g(E[X])$, and we want $E[g(X)]$. Jensen's inequality tells us that the estimate we have for $E[T_n]$ -- as $g(E[X])$ -- is at best a lower bound on the true value. To test how our estimator aligns with what we're looking for, let's run some simulations and see how they line up.

We'll run experiments by building a function that takes some fixed value for $n$, and then for each simulation iteration drawing a sample of size $n$ from an input generating distribution and returning the max value observed in the sample for that iteration. Once all the iterations are complete, we will return statistics on quantiles of interest (especially the median, since I've asserted that that's really what we're interested in) and also the mean, since this is the analytic expectation of the distribution.

```{r}
sim_wait_to_new_max_give_sample_size = function(n,
                                                fx=rnorm,
                                                iters=1e3,
                                                q=c(0.025, .5, .975),
                                                values=FALSE){
  outv=list()
  outv$estimate = matrix(NA_real_, length(n), length(q)+1) #rep(NA_real_, length(n))
  if(values) outv$mat = matrix(NA_real_, length(n), iters)
  for(i in 1:length(n)){
    sim_i = rep(NA_real_, iters)
    for(j in 1:iters){
      x = fx(n[i])
      m0 = max(x)
      m1=m0-1
      k=0
      while(m0>m1){
        k=k+1
        m1=fx(1)
      }
      sim_i[j] = k
    }
    if(values) outv$mat[i,] = sim_i
    outv$estimate[i,] = c(mean(sim_i), quantile(sim_i, q))
  }
  outv
}
```

Now we can run simulations and overlay them with our previous results to see if our estimator has captured the behavior we're looking for.

```{r}
xv=1:20
system.time(test <- sim_wait_to_new_max_give_sample_size(xv, iters=5e3, values=TRUE))
#plot(xv, test$estimate, type='l')
plot(0, 0, type='n', 
     xlim=c(min(xv), max(xv)), 
     ylim=c( min(test$estimate), max(test$estimate[,4]) )
)
lines(xv, test$estimate[,1], col='blue')
lines(xv, test$estimate[,3], lty=2)
lines(xv, test$estimate[,2], col='red', lty=2)
lines(xv, test$estimate[,4], col='red', lty=2)

lines(N, 1/(1-pnorm(CI_true[1,])), col='red')
lines(N, 1/(1-pnorm(CI_true[2,])))
lines(N, 1/(1-pnorm(CI_true[3,])), col='red')
```

Our analytic estimates for the quantiles align very well with the results of the simulation. But, as warned by Jensen's inequality, the simulated expectation for the wait time is actually larger than our naive transformation predicted. So, how do we find the expectation for the transformed random variable?

# Expectations of transformed random variables

