---
title: "A Statistical Argument For Settling: a foray into extreme value theory"
author: "David Marx"
date: "October 10, 2015"
output: html_document
---

# High Standards

The other day I found a post in an online relationship forum in which the author had stumbled across a text conversation between his fiancee and one of her girlfriends he wasn't meant to see. In the conversation, she belittled his sexual performance and talked up her ex. The author of the post was reasonably distressed, but more than feeling disrespected, he insisted that it was extremely important to him to know that he was the best lover she'd ever had. 

Now personally, I don't think that's a fair standard, and I started thinking about how I could construct a statistical argument to demonstrate this. I respect that there is definitely a huge psychological component to the concrete problem at hand, but for the purposes of the thought experiment, let's focus on the math.

# Formalizing the question

Someone has had $m$ sexual partners. Assume it is possible to attribute some kind of rating to each partners' sexual ability, such that we can rank partners relative to this rating and clearly identify the best sexual partner thus far observed. If things don't work out, what is the expected number of new partners needed to be ...observed before the previous "high score" is met or surpassed?

Let's generalize the problem by restating it mathematically. 

1. Let $X \sim f(x;\theta)$ with CDF $F(x;\theta)$
2. Consider an i.i.d. sample of size $n$ drawn from $X$: $\{X_1, X_2, \ldots X_n \}$
3. Let $X_{(n)}$ denote the max observation in the above sample, i.e. $\mathop{\max} (X_1, X_2, \ldots X_n)$

We are interested in finding the expected wait time until we observe a value greater than the previously observed max, *given the number of samples we've already observed*. Concretely:

Find the quantity $E[j|n]$ such that $p(j|n) = p(X_{n+j} \ge X_{(n)} )$

For simplicity, let's assume that we know the concrete value taken by the max (rather than just which observation takes the max value), which we'll denote $m=X_{(n)}$. I'd assert that this isn't really the case in the "best sex partner" question, where we really just have a rank ordering of partners and not a score. Anyway, let's start with this formulation of the problem: we are interested in finding the exected wait time until we observe a value greater than the previously observed max value, *given a concrete score associated with each observation*.

Find the quantity $E[j|m]$ such that $p(j|m) = p(X_{n+j} \ge X_{(n)}| m=X_{(n)} \& X_{n+1, \dots n+j-1} \lt X_{(n)})$

# The Geometric Distribution

If think of the event of finding a new max value as a "success", then we're trying to model the number of observations, $j$, until the first success after a series of failures. In other words, we need the geometric distribution, which has pdf:

$$dgeom(j,p) = p(1-p)^{j-1}$$

where $p$ is the probability of success. This PDF should make a lot of sense intuitively. If $p$ is the success probability, then $1-p$ is the failure probability. We want to know the probability that after $j$ observations, we've had $j-1$ failures and one success. Assuming successes and failures are independent events, we can calculate the joint probability by taking the products of the probabilities of each indpendent event, giving us the geometric PDF described above. 

Since we defined a "success" as achieving a score greater than or equal to the $m$, we can extract this probability from the right tail of the pdf, i.e. the complement of the CDF: $p=1-F(m)$

```{r plot_norm_pdf_tail, echo=FALSE}
xv = seq(-4, 4, .01)
yv = dnorm(xv)
m = 1.5
i=which.max(xv==m)
j=length(xv)
yshift=.1*max(yv)
plot(xv, yv, type='l', xlab="", ylab="", axes=0, ylim=c(-yshift, max(yv)), main="Right tail of PDF = p(x>m)")
polygon(x=c(xv[i], xv[i:j], xv[j]), y= c(0, yv[i:j], 0), col="gray")
#abline(v=m, lty=2)
lines(c(m,m), c(-yshift*.5,max(yv)), lty=2)
#text(m+.5, dnorm(m), 'm')
text(m, -yshift, 'm')
```

# Deriving the expectation of the geometric distribution 

The expectation of the geometric distribution is $\frac{1}{p}$. You can look this up on wikipedia or any math/stats textbook, but just for fun I'm going to derive this expectation here. You can just skip the rest of this section if you're not interested in this derivation.

Let $p(j) = p(1-p)^{j-1}$. 

Let $q=1-p$ such that $p(j) = (1-q)q^{j-1}$

$$
\begin{align}
E[j] &= \sum_{i=1}^{\infty} i (1-q) q^{i-1} \\
     &= \sum_{i=1}^{\infty} i (q^{i-1} - q^i) \\
     &= \sum_{i=1}^{\infty} i q^{i-1} - \sum_{i=1}^{\infty} iq^i \\
     &= \frac{1}{q}\sum_{i=1}^{\infty} i q^i - \sum_{i=1}^{\infty} iq^i \\
     &= \left(\frac{1}{q} - 1\right) \sum_{i=1}^{\infty} iq^i \\
     &= \left(\frac{1}{q} - 1\right) \left( \frac{q}{(1-q)^2} \right) \\
     &= \frac{1}{(1-q)^2} - \frac{q}{(1-q)^2} \\
     &= \frac{1-q}{(1-q)^2} \\
     &= \frac{1}{1-q} \\
     &= \frac{1}{p}
\end{align}
$$


See? That wasn't so bad. Like I said, $E[j]=\frac{1}{p}$.

Back to the problem.

# Expected time to next max value

By translating the problem representation into a geometric random variable, we can infer that $p(j|m)=\frac{1}{1-F(m)}$. This should actually make a lot of sense intuitively. Recall the frequentist interpretation of probability:

$$P(x) \approx \frac{count(\text{# of events})}{count(\text{# of trials})}$$

Let's imagine that for some collection of $n$ samples with $X_{(n)}=m$, $F(m)=\frac{1}{t}$. The frequentist interpretation of probability essentially asserts that our observed max $m$ is a $1$ in $t$ observation, and so it'll probably take another $t$ observations before we see another observation at least as large. 

We've come a long way, but we still have the problem that we don't actually know $m$, and if we did we don't really have access to the CDF. We could construct some sort of measure and try to fit a CDF experimentally, but we can get pretty far analytically.

The missing piece to our solution is an estimator for $m$ given the number of samples we've observed. But, if you think about it, we don't really even need this: what we *really* need, is an estimator for $p$. For this, we can just fall back on the frequentist interpretation of probability. Empirically, the max observation in a sample of $n$ observations is a $1$ in $n$ observation, so our empirical estimate of seeing a value at least as large (its CDF complement) is therefore just $\frac{1}{n}$. 

This leaves us with the very simple heuristic that $E[j|n] = n$

... except this isn't entirely accurate, as we'll learn in a moment.


## Putting it all together

From earlier, we have that the max of an i.i.d. sample of size $n$ with $CDF=F(x)$ is $F(x)^n$. We jumped through a lot of hoops to calculate $E[F(x)^n]$, but it's pretty much trivial to calculate the *median* of this statistic, or any quantile. If we plug the result back into the CDF, we get the probability of observing this quantity given the generating distribution. If we plug the resulting probability into the expected value of the geometric distribution, $\frac{1}{p}$, we get the expected time until we see an observation of at least this magnitude. 

$$X_{(n)} \sim \frac{d}{dx} F_X(x)^n \text{ s.t. } F_{X_{(n)}} = F_X(x)^n$$

$$E[X_{(n)}] \approx F^{-1}_{X_{(n)}}(.5)$$

$$p_n =F_X \left( E[X_{(n)}] \right) =      =F_X \left( F^{-1}_{X_{(n)}}(.5) \right)$$

$$T_n=\frac{1}{1-p_n}=\frac{1}{1 - F_X \left( F^{-1}_{X_{(n)}}(.5) \right)}$$

This statistic $T_n$ gives us an approximation to the expected number of observations until we see a new max value, given the number of observations we've already observed.

```{r analytic_solution1}
xv = seq(-4,4, length.out=1e5)

N=1:20
q=c(.025, .5, .975)
test = sapply(N, function(n) pnorm(xv)^n)
CI_true = apply(test, 2, function(x) sapply(q, function(qi) xv[which.min(qi>x)]))
plot(0, 0, type='n', xlim=c(min(N), max(N)), ylim=c(min(CI_true), ylim=max(CI_true)))
lines(N, CI_true[1,], col='red', lty=2)
lines(N, CI_true[2,])
lines(N, CI_true[3,], col='red', lty=2)
```

```{r analytic_solution2}
CI2 = 1/(1-pnorm(CI_true))
plot(0, 0, type='n', 
     xlim=c(min(N), max(N)), 
     ylim=c( min(CI2), max(CI2) )
     )

lines(N, CI2[1,], col='red', lty=2)
lines(N, CI2[2,])
lines(N, CI2[3,], col='red', lty=2)
```



## (Method 2.) Monte carlo simulation of mean

Let's run some simulations to test our math. To do this, we'll draw a new sample of size $n$ each iteration of the simulation and evaluate the max value for that sample. This will give us a distribution for the max at any given value of $n$ we're interested in.

```{r def_max_given_n}
max_sim_given_n = function(n, reps, fx=rnorm, values=FALSE, mu=TRUE, sigma=TRUE){
  replicate(reps, max(fx(n)))
}
```

Let's visualize this distribution for a few values of $n$ just to see how it behaves.

```{r plot_max_given_n}
plot(0,0, 
     type='n', 
     xlim=c(0,5), 
     ylim=c(0,2), 
     xlab='x',
     ylab='p(x=X_(n))', 
     main="Distribution of max(X_(n))\nfor increasing values of n"
     )
system.time( sapply(1:4,function(x) lines(density(max_sim_given_n(10^x, 5e3)), col=x)) )
legend('topleft', paste("n=10^", 1:4), lty=1, col=1:4  )
```

Now let's run simulations over a full range of $n$ values and visualize the simulated expectation and 95% CI. I'm going to use the median instead of the mean as my simulated expectation to simplify my code a bit. It shouldn't really make much of a difference, but it's worth mentioning since formally monte carlo simulation generally uses the mean.

```{r sim_plot_max_given_n}
#totx=2e3, reps=2e2 -> 85.63 seconds
#totx=1e3, reps=2e2 -> 15.29 seconds
#totx=1e3, resp=1e2 ->  7.8 seconds
#totx=1e2, reps=1e2 ->  0.14 seconds
totx = 1e3 #2e3 
xv_ci = 1:totx
system.time(CI <- sapply(xv_ci, function(x) quantile(max_sim_given_n(x, 2e2), c(.025, .5, .975))))
plot(0,0, type='n', xlab='log(n)', ylab='E[X_(n)]', xlim=c(0,max(log(xv_ci))), ylim=c(0,max(CI)))
lines(log(xv_ci), CI[2,])
lines(log(xv_ci), CI[1,], col='red', lty=2)
lines(log(xv_ci), CI[3,], col='red', lty=2)
```

Let's plot the analytic solution and the simulation results together to see how they line up.

```{r}

plot(0, 0, type='n', 
     xlim=c(min(N), max(N)), 
     ylim=c( min(CI_true), max(CI_true) )
     )

lines(N, CI_true[1,], col='red', lty=2)
lines(N, CI_true[2,])
lines(N, CI_true[3,], col='red', lty=2)

lines(N, CI[2,N], col='blue', lty=2)
lines(N, CI[1,N], col='blue', lty=2)
lines(N, CI[3,N], col='blue', lty=2)
```

Oh baby.

############################

## Simulating the distribution of wait times

There is a famous result in probability called *Jensen's inequality*, which states that:

$$g(E[X]) \le E[g(X)]$$

Above, we found $E[j|X_{(n)}]$ such that $j$ is the index of the next max value to exceed the max value observed for the given sample of size $n$, $X_{(n)}$. Furthermore, we demonstrated that we can use the transformation $g(x) = \frac{1}{1-pnorm(x)}$ to find the wait time to the next max value given some previously observed max value.

We can estimate this waiting time by plugging the analytic solution found above into the $g(x)$ transformation:


```{r}
xv=1:20
yv = 1/(1-pnorm(CI_true))
plot(0, 0, type='n', 
     xlim=c(min(xv), max(xv)), 
     ylim=c(0, max(yv) )
)
lines(N, yv[1,], col='red')
lines(N, yv[2,])
lines(N, yv[3,], col='red')
```

But this is really $g(E[X])$, and we want $E[g(X)]$. Jensen's inequality tells us that the estimate we have for $E[T_n]$ is at best a lower bound on the true value. To test how our estimator aligns with what we're looking for, let's run some simulations and see how they line up.

We'll run experiments by building a function that takes some fixed value for $n$, and then for each simulation iteration drawing a sample of size $n$ from an input generating distribution and returning the max value observed in the sample for that iteration. Once all the iterations are complete, we will return statistics on quantiles of interest (especially the median, since I've asserted that that's really what we're interested in) and also the mean, since this is the analytic expectation of the distribution.

```{r}
sim_wait_to_new_max_give_sample_size = function(n,
                                                fx=rnorm,
                                                iters=1e3,
                                                q=c(0.025, .5, .975),
                                                values=FALSE){
  outv=list()
  outv$estimate = matrix(NA_real_, length(n), length(q)+1) #rep(NA_real_, length(n))
  if(values) outv$mat = matrix(NA_real_, length(n), iters)
  for(i in 1:length(n)){
    sim_i = rep(NA_real_, iters)
    for(j in 1:iters){
      x = fx(n[i])
      m0 = max(x)
      m1=m0-1
      k=0
      while(m0>m1){
        k=k+1
        m1=fx(1)
      }
      sim_i[j] = k
    }
    if(values) outv$mat[i,] = sim_i
    outv$estimate[i,] = c(mean(sim_i), quantile(sim_i, q))
  }
  outv
}
```

Now we can run simulations and overlay them with our previous results to see if our estimator has captured the behavior we're looking for.

```{r}
xv=1:20
system.time(test <- sim_wait_to_new_max_give_sample_size(xv, iters=5e3, values=TRUE))
#plot(xv, test$estimate, type='l')
plot(0, 0, type='n', 
     xlim=c(min(xv), max(xv)), 
     ylim=c( min(test$estimate), max(test$estimate[,4]) )
)
lines(xv, test$estimate[,1], col='blue')
lines(xv, test$estimate[,3], lty=2)
lines(xv, test$estimate[,2], col='red', lty=2)
lines(xv, test$estimate[,4], col='red', lty=2)

lines(N, 1/(1-pnorm(CI_true[1,])), col='red')
lines(N, 1/(1-pnorm(CI_true[2,])))
lines(N, 1/(1-pnorm(CI_true[3,])), col='red')
```

Our estimator works! But it's also worth noting that the simulated mean apears to be larger than the median. In other words, this distribution exhibits significant skew.